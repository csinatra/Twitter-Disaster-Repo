{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/connie/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Labeled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sandy_hurricane = pd.read_csv('../Mike/CrisisLexT6/2012_Sandy_Hurricane/2012_Sandy_Hurricane-ontopic_offtopic.csv')\n",
    "alberta_floods = pd.read_csv('../Mike/CrisisLexT6/2013_Alberta_Floods/2013_Alberta_Floods-ontopic_offtopic.csv')\n",
    "boston_bombing = pd.read_csv('../Mike/CrisisLexT6/2013_Boston_Bombings/2013_Boston_Bombings-ontopic_offtopic.csv')\n",
    "oklahoma_tornado = pd.read_csv('../Mike/CrisisLexT6/2013_Oklahoma_Tornado/2013_Oklahoma_Tornado-ontopic_offtopic.csv')\n",
    "queensland_flood = pd.read_csv('../Mike/CrisisLexT6/2013_Queensland_Floods/2013_Queensland_Floods-ontopic_offtopic.csv')\n",
    "texas_explosion = pd.read_csv('../Mike/CrisisLexT6/2013_West_Texas_Explosion/2013_West_Texas_Explosion-ontopic_offtopic.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sandy Hurricane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'262596552399396864'</td>\n",
       "      <td>I've got enough candles to supply a Mexican fa...</td>\n",
       "      <td>off-topic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'263044104500420609'</td>\n",
       "      <td>Sandy be soooo mad that she be shattering our ...</td>\n",
       "      <td>on-topic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'263309629973491712'</td>\n",
       "      <td>@ibexgirl thankfully Hurricane Waugh played it...</td>\n",
       "      <td>off-topic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'263422851133079552'</td>\n",
       "      <td>@taos you never got that magnificent case of B...</td>\n",
       "      <td>off-topic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'262404311223504896'</td>\n",
       "      <td>I'm at Mad River Bar &amp;amp; Grille (New York, N...</td>\n",
       "      <td>off-topic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tweet id                                              tweet  \\\n",
       "0  '262596552399396864'  I've got enough candles to supply a Mexican fa...   \n",
       "1  '263044104500420609'  Sandy be soooo mad that she be shattering our ...   \n",
       "2  '263309629973491712'  @ibexgirl thankfully Hurricane Waugh played it...   \n",
       "3  '263422851133079552'  @taos you never got that magnificent case of B...   \n",
       "4  '262404311223504896'  I'm at Mad River Bar &amp; Grille (New York, N...   \n",
       "\n",
       "       label  \n",
       "0  off-topic  \n",
       "1   on-topic  \n",
       "2  off-topic  \n",
       "3  off-topic  \n",
       "4  off-topic  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sandy_hurricane.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rename Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sandy_hurricane.rename(columns = {'tweet id': 'id',\n",
    "                                  ' tweet': 'text',\n",
    "                                  ' label': 'label'},\n",
    "                       inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encode column `on-topic` and save as `sandy_hurricane_df`: \n",
    "-  1 = on-topic, tweet is related to a disaster\n",
    "-  0 = off-topic, tweet is *not* related to a disaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sandy_hurricane_df = pd.get_dummies(data=sandy_hurricane, \n",
    "                                    columns=['label'], \n",
    "                                    drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add `type` Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a column to indicate the type of disaster it is. This will be necessary when combining all dataframes later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sandy_hurricane_df['type'] = 'hurricane'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat for all other labeled datasets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alberta Floods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "alberta_floods.rename(columns = {'tweet id': 'id',\n",
    "                                 ' tweet': 'text',\n",
    "                                 ' label': 'label'},\n",
    "                      inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "alberta_floods_df = pd.get_dummies(data=alberta_floods, \n",
    "                                   columns=['label'], \n",
    "                                   drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "alberta_floods_df['type'] = 'flood'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boston Bombing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_bombing.rename(columns = {'tweet id': 'id',\n",
    "                                 ' tweet': 'text',\n",
    "                                 ' label': 'label'},\n",
    "                      inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_bombing_df = pd.get_dummies(data=boston_bombing, \n",
    "                                   columns=['label'], \n",
    "                                   drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_bombing_df['type'] = 'bombing'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oklahoma Tornado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "oklahoma_tornado.rename(columns = {'tweet id': 'id',\n",
    "                                   ' tweet': 'text',\n",
    "                                   ' label': 'label'},\n",
    "                       inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "oklahoma_tornado_df = pd.get_dummies(data=oklahoma_tornado, \n",
    "                                     columns=['label'], \n",
    "                                     drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "oklahoma_tornado_df['type'] = 'tornado'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Queensland Flood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "queensland_flood.rename(columns = {'tweet id': 'id',\n",
    "                                   ' tweet': 'text',\n",
    "                                   ' label': 'label'},\n",
    "                       inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "queensland_flood_df = pd.get_dummies(data=queensland_flood, \n",
    "                                     columns=['label'], \n",
    "                                     drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "queensland_flood_df['type'] = 'flood'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Texas Explosion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "texas_explosion.rename(columns = {'tweet id': 'id',\n",
    "                                  ' tweet': 'text',\n",
    "                                  ' label': 'label'},\n",
    "                       inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "texas_explosion_df = pd.get_dummies(data=texas_explosion, \n",
    "                                    columns=['label'], \n",
    "                                    drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "texas_explosion_df['type'] = 'explosion'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine all processed dataframes into one comprehensive dataframe with each tweet related to its respective disaster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_labeled_df = pd.concat([sandy_hurricane_df, \n",
    "                             alberta_floods_df, \n",
    "                             oklahoma_tornado_df, \n",
    "                             queensland_flood_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40064, 4)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_labeled_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a total of 60,082 observations and 3 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label_on-topic</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'262596552399396864'</td>\n",
       "      <td>I've got enough candles to supply a Mexican fa...</td>\n",
       "      <td>0</td>\n",
       "      <td>hurricane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'263044104500420609'</td>\n",
       "      <td>Sandy be soooo mad that she be shattering our ...</td>\n",
       "      <td>1</td>\n",
       "      <td>hurricane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'263309629973491712'</td>\n",
       "      <td>@ibexgirl thankfully Hurricane Waugh played it...</td>\n",
       "      <td>0</td>\n",
       "      <td>hurricane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'263422851133079552'</td>\n",
       "      <td>@taos you never got that magnificent case of B...</td>\n",
       "      <td>0</td>\n",
       "      <td>hurricane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'262404311223504896'</td>\n",
       "      <td>I'm at Mad River Bar &amp;amp; Grille (New York, N...</td>\n",
       "      <td>0</td>\n",
       "      <td>hurricane</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                                               text  \\\n",
       "0  '262596552399396864'  I've got enough candles to supply a Mexican fa...   \n",
       "1  '263044104500420609'  Sandy be soooo mad that she be shattering our ...   \n",
       "2  '263309629973491712'  @ibexgirl thankfully Hurricane Waugh played it...   \n",
       "3  '263422851133079552'  @taos you never got that magnificent case of B...   \n",
       "4  '262404311223504896'  I'm at Mad River Bar &amp; Grille (New York, N...   \n",
       "\n",
       "   label_on-topic       type  \n",
       "0               0  hurricane  \n",
       "1               1  hurricane  \n",
       "2               0  hurricane  \n",
       "3               0  hurricane  \n",
       "4               0  hurricane  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_labeled_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Text\n",
    "\n",
    "In order to remove any text that will only contribute noise to our model, we will define a function that uses regular expressions to replace certain patterns:\n",
    "\n",
    "-  **Convert all text to lower case**\n",
    "-  **Remove additional white sapce**\n",
    "-  **Remove links:** \n",
    "    -  Links starting with `www.` or `https?://:` are replaced with `URL`. Each link is most likely to be unique to the tweet and won't provide any information in regards to the content overall. \n",
    "-  **Eliminate hashtags**\n",
    "-  **Remove `@`** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processTweet(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',tweet)\n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "    tweet = re.sub('@', '', tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_labeled_df['processed'] = [processTweet(i) for i in final_labeled_df['text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instantiate tokenizer and define the search pattern using `r'\\w+` as our regular expression. We only want to search through words and omit digits and symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tokenize the `processed` column and create a new column (`tokenized`) for our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_labeled_df['tokenized'] = final_labeled_df['processed'].map(lambda x: tokenizer.tokenize(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will lemmatize our data in an attempt to return the base form of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We lemmatize the tokenized words in `tokenized` and join them to represent one string again. This is shown in the `lemmatized` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_labeled_df['lemmatized'] = final_labeled_df['tokenized'].map(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label_on-topic</th>\n",
       "      <th>type</th>\n",
       "      <th>processed</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'262596552399396864'</td>\n",
       "      <td>I've got enough candles to supply a Mexican fa...</td>\n",
       "      <td>0</td>\n",
       "      <td>hurricane</td>\n",
       "      <td>i've got enough candles to supply a mexican fa...</td>\n",
       "      <td>[i, ve, got, enough, candles, to, supply, a, m...</td>\n",
       "      <td>i ve got enough candle to supply a mexican family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'263044104500420609'</td>\n",
       "      <td>Sandy be soooo mad that she be shattering our ...</td>\n",
       "      <td>1</td>\n",
       "      <td>hurricane</td>\n",
       "      <td>sandy be soooo mad that she be shattering our ...</td>\n",
       "      <td>[sandy, be, soooo, mad, that, she, be, shatter...</td>\n",
       "      <td>sandy be soooo mad that she be shattering our ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'263309629973491712'</td>\n",
       "      <td>@ibexgirl thankfully Hurricane Waugh played it...</td>\n",
       "      <td>0</td>\n",
       "      <td>hurricane</td>\n",
       "      <td>ibexgirl thankfully hurricane waugh played it ...</td>\n",
       "      <td>[ibexgirl, thankfully, hurricane, waugh, playe...</td>\n",
       "      <td>ibexgirl thankfully hurricane waugh played it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'263422851133079552'</td>\n",
       "      <td>@taos you never got that magnificent case of B...</td>\n",
       "      <td>0</td>\n",
       "      <td>hurricane</td>\n",
       "      <td>taos you never got that magnificent case of bu...</td>\n",
       "      <td>[taos, you, never, got, that, magnificent, cas...</td>\n",
       "      <td>tao you never got that magnificent case of bur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'262404311223504896'</td>\n",
       "      <td>I'm at Mad River Bar &amp;amp; Grille (New York, N...</td>\n",
       "      <td>0</td>\n",
       "      <td>hurricane</td>\n",
       "      <td>i'm at mad river bar &amp;amp; grille (new york, n...</td>\n",
       "      <td>[i, m, at, mad, river, bar, amp, grille, new, ...</td>\n",
       "      <td>i m at mad river bar amp grille new york ny URL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                                               text  \\\n",
       "0  '262596552399396864'  I've got enough candles to supply a Mexican fa...   \n",
       "1  '263044104500420609'  Sandy be soooo mad that she be shattering our ...   \n",
       "2  '263309629973491712'  @ibexgirl thankfully Hurricane Waugh played it...   \n",
       "3  '263422851133079552'  @taos you never got that magnificent case of B...   \n",
       "4  '262404311223504896'  I'm at Mad River Bar &amp; Grille (New York, N...   \n",
       "\n",
       "   label_on-topic       type  \\\n",
       "0               0  hurricane   \n",
       "1               1  hurricane   \n",
       "2               0  hurricane   \n",
       "3               0  hurricane   \n",
       "4               0  hurricane   \n",
       "\n",
       "                                           processed  \\\n",
       "0  i've got enough candles to supply a mexican fa...   \n",
       "1  sandy be soooo mad that she be shattering our ...   \n",
       "2  ibexgirl thankfully hurricane waugh played it ...   \n",
       "3  taos you never got that magnificent case of bu...   \n",
       "4  i'm at mad river bar &amp; grille (new york, n...   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  [i, ve, got, enough, candles, to, supply, a, m...   \n",
       "1  [sandy, be, soooo, mad, that, she, be, shatter...   \n",
       "2  [ibexgirl, thankfully, hurricane, waugh, playe...   \n",
       "3  [taos, you, never, got, that, magnificent, cas...   \n",
       "4  [i, m, at, mad, river, bar, amp, grille, new, ...   \n",
       "\n",
       "                                          lemmatized  \n",
       "0  i ve got enough candle to supply a mexican family  \n",
       "1  sandy be soooo mad that she be shattering our ...  \n",
       "2  ibexgirl thankfully hurricane waugh played it ...  \n",
       "3  tao you never got that magnificent case of bur...  \n",
       "4    i m at mad river bar amp grille new york ny URL  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_labeled_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to use TF-IDF to determine which words are most discriminating between tweets. Words that occue frequently are penalized and rare words are given more influence in our model. \n",
    "\n",
    "We instantiate TfidfVectorizer() and set:\n",
    "-  `ngram_range = (1, 2)`: Set an upper and lower bound of 1 and 2. Word sequence will contain at least 1 and up to 2 words.\n",
    "-  `stop_words='english'`: Filter out commonly used words in English\n",
    "-  `min_df = 25`: Ignore terms that occur in less than 25 documents of the corpus\n",
    "-  `max_df = 1.0`: There is no maximum threshold since terms cannot have a document frequency greater than `100%`. \n",
    "\n",
    "We then create a dataframe, `tfidf_df` for each word and its frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1,2), \n",
    "                        stop_words = 'english', \n",
    "                        min_df = 25, \n",
    "                        max_df = 1.0)\n",
    "\n",
    "tfidf_df = pd.SparseDataFrame(tfidf.fit_transform(final_labeled_df['lemmatized']),\n",
    "                              columns = tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df.fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>00 humidity</th>\n",
       "      <th>000</th>\n",
       "      <th>000 help</th>\n",
       "      <th>000 home</th>\n",
       "      <th>000 people</th>\n",
       "      <th>10</th>\n",
       "      <th>10 donation</th>\n",
       "      <th>10 online</th>\n",
       "      <th>100</th>\n",
       "      <th>...</th>\n",
       "      <th>yycflood abflood</th>\n",
       "      <th>yycflood relief</th>\n",
       "      <th>yycflood url</th>\n",
       "      <th>yycflood yyc</th>\n",
       "      <th>yycflood yychelps</th>\n",
       "      <th>yycfloods</th>\n",
       "      <th>yychelps</th>\n",
       "      <th>yychelps yycflood</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 2599 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    00  00 humidity  000  000 help  000 home  000 people   10  10 donation  \\\n",
       "0  0.0          0.0  0.0       0.0       0.0         0.0  0.0          0.0   \n",
       "1  0.0          0.0  0.0       0.0       0.0         0.0  0.0          0.0   \n",
       "2  0.0          0.0  0.0       0.0       0.0         0.0  0.0          0.0   \n",
       "3  0.0          0.0  0.0       0.0       0.0         0.0  0.0          0.0   \n",
       "4  0.0          0.0  0.0       0.0       0.0         0.0  0.0          0.0   \n",
       "\n",
       "   10 online  100 ...   yycflood abflood  yycflood relief  yycflood url  \\\n",
       "0        0.0  0.0 ...                0.0              0.0           0.0   \n",
       "1        0.0  0.0 ...                0.0              0.0           0.0   \n",
       "2        0.0  0.0 ...                0.0              0.0           0.0   \n",
       "3        0.0  0.0 ...                0.0              0.0           0.0   \n",
       "4        0.0  0.0 ...                0.0              0.0           0.0   \n",
       "\n",
       "   yycflood yyc  yycflood yychelps  yycfloods  yychelps  yychelps yycflood  \\\n",
       "0           0.0                0.0        0.0       0.0                0.0   \n",
       "1           0.0                0.0        0.0       0.0                0.0   \n",
       "2           0.0                0.0        0.0       0.0                0.0   \n",
       "3           0.0                0.0        0.0       0.0                0.0   \n",
       "4           0.0                0.0        0.0       0.0                0.0   \n",
       "\n",
       "   zone  zoo  \n",
       "0   0.0  0.0  \n",
       "1   0.0  0.0  \n",
       "2   0.0  0.0  \n",
       "3   0.0  0.0  \n",
       "4   0.0  0.0  \n",
       "\n",
       "[5 rows x 2599 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40064, 2599)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singular Value Decomposition (02_EDA_and_Cleaning)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
